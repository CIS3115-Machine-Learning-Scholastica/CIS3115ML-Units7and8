{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# MNIST Tutorial for CIS 4115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== This should generate a FutureWaring on Conversion ===== ignore this warning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "print (\"====== This should generate a FutureWaring on Conversion ===== ignore this warning\")\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Flatten, LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version  1.15.4\n",
      "Keras version  2.2.4\n"
     ]
    }
   ],
   "source": [
    "print (\"Numpy version \" , np.__version__)\n",
    "print (\"Keras version \" , keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the MNIST data\n",
    "Display some sample images and the shape of the data to make sure the import worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n",
      "Kaggle uses a different set of MNIST images so don't load the built in Keras dataset\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "print(\"Kaggle uses a different set of MNIST images so don't load the built in Keras dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the actual Kaggle download files instead\n",
    "# Pull out the labels or output which are saved in first index\n",
    "# Convert remaining values to floats\n",
    "train = pd.read_csv('train.csv')\n",
    "Y_train_kaggle = train.iloc[:,0].values.astype('int32')\n",
    "X_train_kaggle = (train.iloc[:,1:].values).astype('float32')\n",
    "X_test_kaggle = (pd.read_csv('test.csv').values).astype('float32')\n",
    "#reshape as 28x28 pixel images\n",
    "X_train_kaggle = X_train_kaggle.reshape(X_train_kaggle.shape[0], 28, 28)\n",
    "X_test_kaggle = X_test_kaggle.reshape(X_test_kaggle.shape[0], 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train training data shape of 28x28 pixels greyscale:  (60000, 28, 28)\n",
      "X_test training data shape of 28x28 pixels greyscale: :  (10000, 28, 28)\n",
      "X_train_kaggle training data shape of 28x28 pixels greyscale:  (42000, 28, 28)\n",
      "X_test_kaggle training data shape of 28x28 pixels greyscale: :  (28000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print (\"X_train training data shape of 28x28 pixels greyscale: \" ,X_train.shape)\n",
    "print (\"X_test training data shape of 28x28 pixels greyscale: : \" ,X_test.shape)\n",
    "print (\"X_train_kaggle training data shape of 28x28 pixels greyscale: \" ,X_train_kaggle.shape)\n",
    "print (\"X_test_kaggle training data shape of 28x28 pixels greyscale: : \" ,X_test_kaggle.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to display a sample of any image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28cc45dcf60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAD5tJREFUeJzt3X+QVfV5x/HPIyDERSIEQYqIxmDVkPijixitCRmrIS0NGKOFyVScOm5qYqMTncY6qdKZ2jiZatQY02DDiJ0ImvqLTG0i2SajaXRlVSrqaqAO6soGsNgsiiw/9ukfe0hX3PO9y73n/th93q8Z5t57nnPuebzy4dx7v/ecr7m7AMRzUL0bAFAfhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFAja7mzg220j1FTLXcJhLJT72iX99hg1q0o/GY2V9KtkkZI+md3vzG1/hg1abadXckuASS0eeug1y37bb+ZjZD0XUmflXSipEVmdmK5zwegtir5zH+apA3u/oq775K0UtL8YtoCUG2VhH+qpNf7Pe7Mlr2HmbWYWbuZte9WTwW7A1CkSsI/0JcK7zs/2N2XunuzuzeP0ugKdgegSJWEv1PStH6Pj5S0qbJ2ANRKJeFfI2mGmR1jZgdLWihpVTFtAai2sof63H2PmV0u6afqG+pb5u4vFNYZgKqqaJzf3R+R9EhBvQCoIX7eCwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFAVzdJrZhslbZe0V9Ied28uoikA1VdR+DOfdvc3C3geADXE234gqErD75IeNbOnzayliIYA1Ealb/vPdPdNZjZJ0moze8ndH+u/QvaPQoskjdEhFe4OQFEqOvK7+6bsdoukByWdNsA6S9292d2bR2l0JbsDUKCyw29mTWZ26L77ks6V9HxRjQGorkre9k+W9KCZ7Xuee9z9J4V0BaDqyg6/u78i6aQCewlr5LQjk/WOq9P1M2a9lFtbPv0/yuppn3d9V7I+78WFyfqrGybl1satr+wrp6n3bEjWe7f9b27Nd6f/uyJgqA8IivADQRF+ICjCDwRF+IGgCD8QVBFn9YUw4iPH5NZeO39Kctvj5/06Wb/3w6vK6mmf7t6dubX730n3VsoY252st370gfQTfLSi3addnS7//Zszc2srH5yT3Hb6De3J+nAYKuTIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc4/SGc80JFb+/GH7q/ouc/tWJCsd7ZNTdaPeXB7bs3bK7u+yq65s5L1s+68NV2/PX8wfsqT75bV0z5dn/hAsj5rwbrc2vOX3p7c9iOT/jJZP+7LTyXrQwFHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iyty9ZjsbZxN8tp1ds/0VacvlZ+TWdn4ove3RD+dfQlqSete+WE5LNdG96PRk/d2J6ePH5O/8qsh2DsiI8eNzay1PrUluu75ncrLe+gcTk3Xv6UnWq6XNW9Xt22ww63LkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgSp7Pb2bLJM2TtMXdZ2bLJki6V9LRkjZKutDd36pem/U36fbyx6t7C+yj1sateDJdr+K+93761GT9vxeOSNZ/OveW3NqxI9PXAvjUNV9M1pt62pL1oWAwR/67JM3db9k1klrdfYak1uwxgCGkZPjd/TFJ2/ZbPF/S8uz+cknpS9EAaDjlfuaf7O5dkpTdTiquJQC1UPVr+JlZi6QWSRqjQ6q9OwCDVO6Rf7OZTZGk7HZL3oruvtTdm929eZRGl7k7AEUrN/yrJC3O7i+W9HAx7QColZLhN7MVkp6Q9Ptm1mlml0i6UdI5ZrZe0jnZYwBDSMnP/O6+KKc0NE/MR6EOOiT9Pc6G607KrS05777ktgvHPpOsd+3dkaz/XddncmtvzEv33bR16I/jl8Iv/ICgCD8QFOEHgiL8QFCEHwiK8ANBMUV3AQ5qakrWX70yf7hLknxUkd281wd+k740+7tHpK/yvHPq7mT9+rPSv++a3/Sz3NoXXl6Y3PaO709J1g97ojNZ39P5RqL6TnLbCDjyA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPMXYNv5H0/Wr7toRbJ+wdj/KbKdQv3bjrHJ+t8suzhZX7m6O7c2sv355LZj9VqyvidZRSkc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKHNPn+9dpHE2wWdbvCt+jxhXYiLraenz1qvppZbxyfpfn/PjZP3Zt49K1l+56vjc2kGPP5vcFgeuzVvV7dvSF2nIcOQHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaBKjvOb2TJJ8yRtcfeZ2bIlki6VtDVb7Vp3f6TUzqKO8w9lB40Zk6xv/Pqpyfr3F9+RW/vG+vOS2x7yuU3Juvf0JOsRFT3Of5ekuQMs/7a7n5z9KRl8AI2lZPjd/TFJ22rQC4AaquQz/+Vm9pyZLTOz9G9EATSccsP/PUnHSjpZUpekm/JWNLMWM2s3s/bd4jMa0CjKCr+7b3b3ve7eK+lOSacl1l3q7s3u3jxKo8vtE0DBygq/mfU/De08SenLsAJoOCUv3W1mKyTNkTTRzDolXS9pjpmdLMklbZT0pSr2CKAKOJ8/03vWKcn6wRu6cmt7un5TdDvDhs36WG7taytWJrf9z3eOS9afuig9X0Lvf3Uk68MR5/MDKInwA0ERfiAowg8ERfiBoAg/EFSYob6R06cl69f/4oFk/W8XXZJffPK5cloKb9fcWcn6/JtWJ+szRqeHWL/zZ+fn1vzpF5LbDlUM9QEoifADQRF+ICjCDwRF+IGgCD8QFOEHgip5Pv9w0fn59Dj/RWv+Ilmfzlh+4Q7+yZpk/dFX078DGHP/r5L1w27LPw37t3MPTW7bu317sj4ccOQHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaDCjPOX0tPNbEKNZm/H+mT9zls/l6w/dd13c2ufnHtZctuxP2pL1ocDjvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTJcX4zmybpbklHSOqVtNTdbzWzCZLulXS0pI2SLnT3t6rXamWaunqT9bkXtSfr6w4/PLe2d+vWsnpCZSYtfzZZ/6crpufWNi/Yldx27I/KamlIGcyRf4+kq9z9BEmnS/qKmZ0o6RpJre4+Q1Jr9hjAEFEy/O7e5e7PZPe3S+qQNFXSfEnLs9WWS1pQrSYBFO+APvOb2dGSTpHUJmmyu3dJff9ASJpUdHMAqmfQ4TezsZLul3Slu3cfwHYtZtZuZu271VNOjwCqYFDhN7NR6gv+D91934yWm81sSlafImnLQNu6+1J3b3b35lHi5BmgUZQMv5mZpB9I6nD3m/uVVklanN1fLOnh4tsDUC2DOaX3TEl/Lmmdma3Nll0r6UZJ95nZJZJek3RBdVosxmGPvJis/8k/rE3Wf/b5/FNAJ965Lb3z3r3pOsrSu3Nnst7222Nyaxd/7Inkto9rTFk9DSUlw+/uv5SUN9/32cW2A6BW+IUfEBThB4Ii/EBQhB8IivADQRF+IKgwl+7e253+RfJXb/lysv7zb/xjbq35hK8ltz3u2vT03r07diTrGNjGGz6RrN885ebc2hfuuiq57VFKT/89HHDkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgwozzlzL5tvS47hy7OrfWfnX+bwAkaenZJyXrD30zfWb0+H9/OVnf+1bDXjE97fSPJ8vrL0v/9ez4o9uS9eNXXZFfu2ldctv0hd6HB478QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxCUuXvNdjbOJvhsG35X++791CnJ+qavpqeD/qsTfpGsHz+6K1m/7Jkv5tY++FBTcttRO9L//7uPGpGs75idvhbBjc0P5NY+c8iAkzz9zvLuGcn6Hf/yp8n6kd8c/ufk76/NW9Xt2/Iutf8eHPmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiS4/xmNk3S3ZKOUN9pzkvd/VYzWyLpUklbs1WvdfdHUs81XMf5KzXisA8m6y8vOSFZnz0r/3z/K3/v0eS2m/aMT9YXNL2drF/6+pnJemv7zNzapCfSw9Hj/3Vtst67c2eyHtGBjPMP5mIeeyRd5e7PmNmhkp42s9VZ7dvunr6SBYCGVDL87t4lqSu7v93MOiRNrXZjAKrrgD7zm9nRkk6R1JYtutzMnjOzZWY24PtHM2sxs3Yza9+tnoqaBVCcQYffzMZKul/Sle7eLel7ko6VdLL63hncNNB27r7U3ZvdvXmURhfQMoAiDCr8ZjZKfcH/obs/IEnuvtnd97p7r6Q7JZ1WvTYBFK1k+M3MJP1AUoe739xv+ZR+q50n6fni2wNQLYMZ6vtDSY9LWqf/v6LxtZIWqe8tv0vaKOlL2ZeDuRjqA6qr0KE+d/+lpIGeLDmmD6Cx8Qs/ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUDWdotvMtkp6td+iiZLerFkDB6ZRe2vUviR6K1eRvU1398MHs2JNw/++nZu1u3tz3RpIaNTeGrUvid7KVa/eeNsPBEX4gaDqHf6ldd5/SqP21qh9SfRWrrr0VtfP/ADqp95HfgB1Upfwm9lcM3vZzDaY2TX16CGPmW00s3VmttbM2uvcyzIz22Jmz/dbNsHMVpvZ+uw2Pc1ubXtbYmZvZK/dWjP74zr1Ns3Mfm5mHWb2gpldkS2v62uX6Ksur1vN3/ab2QhJv5Z0jqROSWskLXL3F2vaSA4z2yip2d3rPiZsZp+U9Laku919ZrbsW5K2ufuN2T+c49396w3S2xJJb9d75uZsQpkp/WeWlrRA0sWq42uX6OtC1eF1q8eR/zRJG9z9FXffJWmlpPl16KPhuftjkrbtt3i+pOXZ/eXq+8tTczm9NQR373L3Z7L72yXtm1m6rq9doq+6qEf4p0p6vd/jTjXWlN8u6VEze9rMWurdzAAm75sZKbudVOd+9ldy5uZa2m9m6YZ57cqZ8bpo9Qj/QLP/NNKQw5nufqqkz0r6Svb2FoMzqJmba2WAmaUbQrkzXhetHuHvlDSt3+MjJW2qQx8DcvdN2e0WSQ+q8WYf3rxvktTsdkud+/mdRpq5eaCZpdUAr10jzXhdj/CvkTTDzI4xs4MlLZS0qg59vI+ZNWVfxMjMmiSdq8abfXiVpMXZ/cWSHq5jL+/RKDM3580srTq/do0243VdfuSTDWXcImmEpGXufkPNmxiAmX1YfUd7qW8S03vq2ZuZrZA0R31nfW2WdL2khyTdJ+koSa9JusDda/7FW05vc3SAMzdXqbe8maXbVMfXrsgZrwvph1/4ATHxCz8gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0H9H5ZKkIJoaLd9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train_kaggle [7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize image data\n",
    "Greyscale pixel data is stored as integer values between 0-255.\n",
    "<br>\n",
    "Neural networks expect inputs between 0-1, so divide each pixel by 255 to normalize it. Also change the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First image data before normalizing\")\n",
    "print (X_train[:1])\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "#Do the same for the kaggle input\n",
    "X_train_kaggle = X_train_kaggle.astype('float32')\n",
    "X_test_kaggle = X_test_kaggle.astype('float32')\n",
    "X_train_kaggle = X_train_kaggle / 255\n",
    "X_test_kaggle = X_test_kaggle / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape data\n",
    "Keras expects inputs with three values for images. Generally the x, y, and depth of the image. So a 3x4 image \n",
    "Data we are starting with with shape of (3, 4) <br>\n",
    "[ [ 9 9 9 ] <br>\n",
    "&nbsp;&nbsp;[ 9 9 9 ] <br>\n",
    "&nbsp;&nbsp;[ 9 9 9 ] <br>\n",
    "&nbsp;&nbsp;[ 9 9 9 ] ] <br>\n",
    "Data format after reshape that keras needs with with shape of (3, 4, 1) <br>\n",
    "[ [ [ 9 9 9 ] <br>\n",
    "&nbsp;&nbsp;&nbsp;[ 9 9 9 ] <br>\n",
    "&nbsp;&nbsp;&nbsp;[ 9 9 9 ] <br>\n",
    "&nbsp;&nbsp;&nbsp;[ 9 9 9 ] ] ]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Shape of training input before reshaping \", X_train.shape)\n",
    "train_size = X_train.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "X_train = X_train.reshape(train_size, 28, 28, 1)\n",
    "X_test = X_test.reshape(test_size, 28, 28, 1)\n",
    "X_train_kaggle = X_train_kaggle.reshape(X_train_kaggle.shape[0], 28, 28, 1)\n",
    "X_test_kaggle = X_test_kaggle.reshape(X_test_kaggle.shape[0], 28, 28, 1)\n",
    "print (\"Shape of training input after reshaping \", X_train.shape)\n",
    "\n",
    "#reshape for dense-only inputs\n",
    "X_train_dense = X_train.reshape(train_size, 28 * 28)\n",
    "X_test_dense = X_test.reshape(test_size, 28 * 28)\n",
    "X_train_kaggle_dense = X_train_kaggle.reshape(X_train_kaggle.shape[0], 28 * 28)\n",
    "X_test_kaggle_dense = X_test_kaggle.reshape(X_test_kaggle.shape[0], 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the output data\n",
    "\n",
    "Output data is stored in arrays named \"y\" or y_train and y_test. <br>\n",
    "Initiailly this is just the number, 0-9, that is represented by the image. <br>\n",
    "Output should be an array of 10 different values each 0 or 1. <br>\n",
    "So, convert 4 into [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"First 10 entries in kaggle output data before reformatting \",Y_train_kaggle[:10])\n",
    "print (\"First 10 entries in normtal output data before reformatting \",Y_train[:10])\n",
    "print (\"Shape of Y_train \",Y_train.shape)\n",
    "print (\"Shape of Y_test \",Y_test.shape)\n",
    "print (\"Shape of Y_train_kaggle \",Y_train_kaggle.shape)\n",
    "print (\"No Y_test_kaggle is availab because it is a competition and they don't give out the answers... \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "Y_train_kaggle = np_utils.to_categorical(Y_train_kaggle, 10)\n",
    "# Kaggle does not provide the testing results--we must generate and submit these\n",
    "print (\"Shape of output data after reformatting \",Y_train.shape)\n",
    "print (\"First 10 entries in output data after reformatting \")\n",
    "print (Y_train[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Neural Network\n",
    "First, define the neural network layers in the model <br>\n",
    "Second, compile the model to build it <br>\n",
    "Third, train the model using the fit command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and train simple 3-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model created\n"
     ]
    }
   ],
   "source": [
    "Simple_3layer_model = Sequential()\n",
    "Simple_3layer_model.add(Dense(32, activation='relu', input_dim=(28*28)))\n",
    "Simple_3layer_model.add(Dense(32, activation='relu'))\n",
    "Simple_3layer_model.add(Dense(10, activation='softmax'))\n",
    "print (\"Neural Network Model created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Simple_3layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "Simple_3layer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      " - 4s - loss: 1.3685 - acc: 0.6022 - val_loss: 0.7637 - val_acc: 0.7658\n",
      "Epoch 2/5\n",
      " - 3s - loss: 0.6555 - acc: 0.8008 - val_loss: 0.5493 - val_acc: 0.8349\n",
      "Epoch 3/5\n",
      " - 3s - loss: 0.5053 - acc: 0.8511 - val_loss: 0.4509 - val_acc: 0.8668\n",
      "Epoch 4/5\n",
      " - 3s - loss: 0.4317 - acc: 0.8737 - val_loss: 0.3993 - val_acc: 0.8812\n",
      "Epoch 5/5\n",
      " - 3s - loss: 0.3909 - acc: 0.8868 - val_loss: 0.3679 - val_acc: 0.8942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x277d0862c88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fit model on standard MNIST data which includes validation data \n",
    "# *** RUN MORE EPOCHS WHEN TESTING FOR REAL ***\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "Simple_3layer_model.fit(X_train_dense, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test_dense, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 3s - loss: 0.0994 - acc: 0.9697\n",
      "Epoch 2/5\n",
      " - 3s - loss: 0.0868 - acc: 0.9742\n",
      "Epoch 3/5\n",
      " - 3s - loss: 0.0790 - acc: 0.9760\n",
      "Epoch 4/5\n",
      " - 3s - loss: 0.0717 - acc: 0.9784\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.0642 - acc: 0.9804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22806a79cf8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fit model on Kaggle MNIST data which does not include validation data \n",
    "# *** RUN MORE EPOCHS WHEN TESTING FOR REAL ***\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "Simple_3layer_model.fit(X_train_kaggle_dense, Y_train_kaggle,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and train CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model created\n"
     ]
    }
   ],
   "source": [
    "CNNmodel = Sequential()\n",
    "CNNmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "CNNmodel.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "CNNmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNNmodel.add(Dropout(0.25))\n",
    "CNNmodel.add(Flatten())\n",
    "CNNmodel.add(Dense(32, activation='relu'))\n",
    "CNNmodel.add(Dense(10, activation='softmax'))\n",
    "print (\"Neural Network Model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "CNNmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                73760     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 76,570\n",
      "Trainable params: 76,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = CNNmodel.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "second_layer_weights = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (60000, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b7af7109777c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m           validation_data=(X_test, Y_test))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1637\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1638\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (60000, 28, 28)"
     ]
    }
   ],
   "source": [
    "#  Fit the CNN model on standard MNIST data which includes validation data \n",
    "# *** RUN MORE EPOCHS WHEN TESTING FOR REAL ***\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "CNN_hist1 = CNNmodel.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 56s - loss: 0.0808 - acc: 0.9751\n",
      "Epoch 2/5\n",
      " - 56s - loss: 0.0769 - acc: 0.9763\n",
      "Epoch 3/5\n",
      " - 56s - loss: 0.0730 - acc: 0.9770\n",
      "Epoch 4/5\n",
      " - 55s - loss: 0.0701 - acc: 0.9776\n",
      "Epoch 5/5\n",
      " - 56s - loss: 0.0663 - acc: 0.9787\n"
     ]
    }
   ],
   "source": [
    "#  Fit CNN model on Kaggle MNIST data which does not include validation data\n",
    "# *** RUN MORE EPOCHS WHEN TESTING FOR REAL ***\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "CNN_hist2 = CNNmodel.fit(X_train_kaggle, Y_train_kaggle,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the simple 3-layer network on the test data\n",
      "[0.36790424445867537, 0.8942]\n",
      "Evaluate the CNN network on the test data\n",
      "[0.044538643525261434, 0.9847]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "print (\"Evaluate the simple 3-layer network on the test data\")\n",
    "S3ln_Score = Simple_3layer_model.evaluate(X_test_dense, Y_test, verbose=2)\n",
    "print (S3ln_Score)\n",
    "print (\"Evaluate the CNN network on the test data\")\n",
    "CNN_Score = CNNmodel.evaluate(X_test, Y_test, verbose=2)\n",
    "print (CNN_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource of image augmentation: https://machinelearningmastery.com/image-augmentation-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making this generator beforehand\n",
    "imdgen = ImageDataGenerator(\n",
    "    featurewise_center = False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center = False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization = False,  # divide each input by its std\n",
    "    zca_whitening = False,  # apply ZCA whitening\n",
    "    rotation_range = 10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range = 0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range = 0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip = False,  # randomly flip images\n",
    "    vertical_flip = False,  # randomly flip images\n",
    "    shear_range=0.1,       \n",
    "    zoom_range=0.1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "imdgen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the batches generated by datagen.flow()\n",
    "dgen = imdgen.flow(X_train, Y_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 87s - loss: 0.3566 - acc: 0.8909\n",
      "Epoch 2/5\n",
      " - 93s - loss: 0.2623 - acc: 0.9199\n",
      "Epoch 3/5\n",
      " - 89s - loss: 0.2306 - acc: 0.9290\n",
      "Epoch 4/5\n",
      " - 104s - loss: 0.2093 - acc: 0.9351\n",
      "Epoch 5/5\n",
      " - 100s - loss: 0.1988 - acc: 0.9395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27781a8b4a8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fit CNN model on MNIST data with image augmentation\n",
    "# *** RUN MORE EPOCHS WHEN TESTING FOR REAL ***\n",
    "epochs = 5\n",
    "\n",
    "CNNmodel.fit_generator(dgen, \n",
    "          epochs=epochs,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 84s - loss: 0.1853 - acc: 0.9436\n",
      "Epoch 2/50\n",
      " - 85s - loss: 0.1754 - acc: 0.9461\n",
      "Epoch 3/50\n",
      " - 85s - loss: 0.1666 - acc: 0.9490\n",
      "Epoch 4/50\n",
      " - 85s - loss: 0.1627 - acc: 0.9504\n",
      "Epoch 5/50\n",
      " - 84s - loss: 0.1566 - acc: 0.9529\n",
      "Epoch 6/50\n",
      " - 84s - loss: 0.1552 - acc: 0.9527\n",
      "Epoch 7/50\n",
      " - 86s - loss: 0.1484 - acc: 0.9555\n",
      "Epoch 8/50\n",
      " - 84s - loss: 0.1432 - acc: 0.9567\n",
      "Epoch 9/50\n",
      " - 84s - loss: 0.1391 - acc: 0.9574\n",
      "Epoch 10/50\n",
      " - 84s - loss: 0.1364 - acc: 0.9574\n",
      "Epoch 11/50\n",
      " - 84s - loss: 0.1348 - acc: 0.9589\n",
      "Epoch 12/50\n",
      " - 84s - loss: 0.1325 - acc: 0.9605\n",
      "Epoch 13/50\n",
      " - 84s - loss: 0.1310 - acc: 0.9604\n",
      "Epoch 14/50\n",
      " - 83s - loss: 0.1273 - acc: 0.9616\n",
      "Epoch 15/50\n",
      " - 84s - loss: 0.1247 - acc: 0.9613\n",
      "Epoch 16/50\n",
      " - 83s - loss: 0.1247 - acc: 0.9609\n",
      "Epoch 17/50\n",
      " - 84s - loss: 0.1215 - acc: 0.9632\n",
      "Epoch 18/50\n",
      " - 83s - loss: 0.1233 - acc: 0.9618\n",
      "Epoch 19/50\n",
      " - 84s - loss: 0.1187 - acc: 0.9644\n",
      "Epoch 20/50\n",
      " - 84s - loss: 0.1168 - acc: 0.9647\n",
      "Epoch 21/50\n",
      " - 83s - loss: 0.1177 - acc: 0.9641\n",
      "Epoch 22/50\n",
      " - 84s - loss: 0.1145 - acc: 0.9654\n",
      "Epoch 23/50\n",
      " - 83s - loss: 0.1117 - acc: 0.9655\n",
      "Epoch 24/50\n",
      " - 83s - loss: 0.1088 - acc: 0.9670\n",
      "Epoch 25/50\n",
      " - 84s - loss: 0.1106 - acc: 0.9667\n",
      "Epoch 26/50\n",
      " - 84s - loss: 0.1086 - acc: 0.9669\n",
      "Epoch 27/50\n",
      " - 83s - loss: 0.1092 - acc: 0.9665\n",
      "Epoch 28/50\n",
      " - 83s - loss: 0.1088 - acc: 0.9672\n",
      "Epoch 29/50\n",
      " - 83s - loss: 0.1074 - acc: 0.9663\n",
      "Epoch 30/50\n",
      " - 83s - loss: 0.1073 - acc: 0.9675\n",
      "Epoch 31/50\n",
      " - 83s - loss: 0.1039 - acc: 0.9681\n",
      "Epoch 32/50\n",
      " - 83s - loss: 0.1037 - acc: 0.9685\n",
      "Epoch 33/50\n",
      " - 83s - loss: 0.1028 - acc: 0.9688\n",
      "Epoch 34/50\n",
      " - 84s - loss: 0.1036 - acc: 0.9682\n",
      "Epoch 35/50\n",
      " - 83s - loss: 0.1024 - acc: 0.9685\n",
      "Epoch 36/50\n",
      " - 83s - loss: 0.1020 - acc: 0.9689\n",
      "Epoch 37/50\n",
      " - 83s - loss: 0.1031 - acc: 0.9686\n",
      "Epoch 38/50\n",
      " - 83s - loss: 0.1007 - acc: 0.9689\n",
      "Epoch 39/50\n",
      " - 83s - loss: 0.0989 - acc: 0.9702\n",
      "Epoch 40/50\n",
      " - 83s - loss: 0.0999 - acc: 0.9687\n",
      "Epoch 41/50\n",
      " - 83s - loss: 0.0962 - acc: 0.9699\n",
      "Epoch 42/50\n",
      " - 83s - loss: 0.0964 - acc: 0.9705\n",
      "Epoch 43/50\n",
      " - 83s - loss: 0.0987 - acc: 0.9699\n",
      "Epoch 44/50\n",
      " - 84s - loss: 0.0980 - acc: 0.9703\n",
      "Epoch 45/50\n",
      " - 83s - loss: 0.0960 - acc: 0.9701\n",
      "Epoch 46/50\n",
      " - 83s - loss: 0.0939 - acc: 0.9712\n",
      "Epoch 47/50\n",
      " - 84s - loss: 0.0965 - acc: 0.9707\n",
      "Epoch 48/50\n",
      " - 83s - loss: 0.0946 - acc: 0.9715\n",
      "Epoch 49/50\n",
      " - 84s - loss: 0.0941 - acc: 0.9709\n",
      "Epoch 50/50\n",
      " - 83s - loss: 0.0927 - acc: 0.9717\n"
     ]
    }
   ],
   "source": [
    "#  Fit CNN model on MNIST data with image augmentation\n",
    "# *** RUN MORE EPOCHS WHEN TESTING FOR REAL ***\n",
    "epochs = 50\n",
    "\n",
    "CNN_hist3 = CNNmodel.fit_generator(dgen, \n",
    "          epochs=epochs,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the CNN network on the test data\n",
      "[0.03469952181642875, 0.9886]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "print (\"Evaluate the CNN network on the test data\")\n",
    "CNN_Score = CNNmodel.evaluate(X_test, Y_test, verbose=2)\n",
    "print (CNN_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model created\n"
     ]
    }
   ],
   "source": [
    "    CNNmodel2 = Sequential()\n",
    "    CNNmodel2.add(Conv2D(32, 5, activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "    CNNmodel2.add(Conv2D(32, 5, activation=\"relu\"))\n",
    "    CNNmodel2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    CNNmodel2.add(Dropout(0.4))\n",
    "    \n",
    "    CNNmodel2.add(Conv2D(64, 3, activation=\"relu\", padding='same'))\n",
    "    CNNmodel2.add(Conv2D(64, 3, activation=\"relu\"))\n",
    "    CNNmodel2.add(Dropout(0.4))\n",
    "    \n",
    "    CNNmodel2.add(Conv2D(128, 3, activation=\"relu\", padding='same'))\n",
    "    CNNmodel2.add(Conv2D(128, 3, activation=\"relu\"))\n",
    "    CNNmodel2.add(Dropout(0.4))\n",
    "    \n",
    "    CNNmodel2.add(Flatten())\n",
    "    CNNmodel2.add(Dense(256, activation=\"relu\"))\n",
    "    CNNmodel2.add(Dropout(0.5))\n",
    "    CNNmodel2.add(Dense(64, activation=\"relu\"))\n",
    "    CNNmodel2.add(Dropout(0.2))\n",
    "    CNNmodel2.add(Dense(10, activation=\"softmax\"))\n",
    "    print (\"Neural Network Model created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 20, 20, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 6, 6, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,500,330\n",
      "Trainable params: 1,500,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "CNNmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "CNNmodel2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=2, \n",
    "                                            factor=0.4, \n",
    "                                            min_lr=3e-6)\n",
    "early_stops = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, verbose=2, mode='auto')\n",
    "data_aug = ImageDataGenerator(rotation_range=20, width_shift_range=4, height_shift_range=4, zoom_range=0.1)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "dgen = data_aug.flow(X_train, Y_train, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 357s - loss: 2.3015 - acc: 0.1133 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 2/50\n",
      " - 342s - loss: 2.3017 - acc: 0.1108 - val_loss: 2.3015 - val_acc: 0.1135\n",
      "Epoch 3/50\n",
      " - 331s - loss: 2.3016 - acc: 0.1130 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 4/50\n",
      " - 346s - loss: 2.3013 - acc: 0.1115 - val_loss: 2.3012 - val_acc: 0.1135\n",
      "Epoch 5/50\n",
      " - 345s - loss: 2.3013 - acc: 0.1141 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "Epoch 6/50\n",
      " - 337s - loss: 2.3014 - acc: 0.1103 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 7/50\n",
      " - 339s - loss: 2.3012 - acc: 0.1129 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "#  Fit CNN model on MNIST data with image augmentation\n",
    "# *** RUN MORE EPOCHS WHEN TESTING FOR REAL ***\n",
    "epochs = 50\n",
    "CNN_hist4 = CNNmodel2.fit_generator(dgen, steps_per_epoch=len(X_train)//128,\n",
    "                              validation_data=(X_test, Y_test), epochs=epochs, verbose=2, callbacks=[learning_rate_reduction, early_stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 265s - loss: 2.3014 - acc: 0.1132 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 2/50\n",
      " - 235s - loss: 2.3010 - acc: 0.1131 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 3/50\n",
      " - 229s - loss: 2.3013 - acc: 0.1107 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 4/50\n",
      " - 230s - loss: 2.3012 - acc: 0.1132 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 5/50\n",
      " - 231s - loss: 2.3015 - acc: 0.1101 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 6/50\n",
      " - 227s - loss: 2.3012 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 7/50\n",
      " - 227s - loss: 2.3009 - acc: 0.1149 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 8/50\n",
      " - 226s - loss: 2.3014 - acc: 0.1125 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 9/50\n",
      " - 226s - loss: 2.3011 - acc: 0.1108 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 10/50\n",
      " - 226s - loss: 2.3011 - acc: 0.1127 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 11/50\n",
      " - 226s - loss: 2.3012 - acc: 0.1151 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 12/50\n",
      " - 226s - loss: 2.3013 - acc: 0.1111 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 13/50\n",
      " - 226s - loss: 2.3015 - acc: 0.1080 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 14/50\n",
      " - 226s - loss: 2.3010 - acc: 0.1141 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 15/50\n",
      " - 226s - loss: 2.3011 - acc: 0.1158 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 16/50\n",
      " - 227s - loss: 2.3008 - acc: 0.1122 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 17/50\n",
      " - 226s - loss: 2.3017 - acc: 0.1109 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 18/50\n",
      " - 226s - loss: 2.3016 - acc: 0.1127 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 19/50\n",
      " - 226s - loss: 2.3011 - acc: 0.1113 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 20/50\n",
      " - 228s - loss: 2.3013 - acc: 0.1126 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 21/50\n",
      " - 226s - loss: 2.3015 - acc: 0.1097 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 22/50\n",
      " - 226s - loss: 2.3011 - acc: 0.1147 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 23/50\n",
      " - 227s - loss: 2.3009 - acc: 0.1129 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 24/50\n",
      " - 226s - loss: 2.3011 - acc: 0.1139 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 25/50\n",
      " - 226s - loss: 2.3013 - acc: 0.1102 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 26/50\n",
      " - 225s - loss: 2.3015 - acc: 0.1135 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 27/50\n",
      " - 227s - loss: 2.3010 - acc: 0.1142 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 28/50\n",
      " - 226s - loss: 2.3012 - acc: 0.1122 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 29/50\n",
      " - 226s - loss: 2.3013 - acc: 0.1120 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 30/50\n",
      " - 226s - loss: 2.3014 - acc: 0.1108 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 31/50\n",
      " - 226s - loss: 2.3016 - acc: 0.1102 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 32/50\n",
      " - 226s - loss: 2.3008 - acc: 0.1139 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 33/50\n",
      " - 226s - loss: 2.3014 - acc: 0.1106 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 34/50\n",
      " - 226s - loss: 2.3014 - acc: 0.1133 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 35/50\n",
      " - 226s - loss: 2.3009 - acc: 0.1145 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 36/50\n",
      " - 227s - loss: 2.3013 - acc: 0.1125 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 37/50\n",
      " - 228s - loss: 2.3009 - acc: 0.1135 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 38/50\n",
      " - 226s - loss: 2.3014 - acc: 0.1090 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 39/50\n",
      " - 227s - loss: 2.3015 - acc: 0.1133 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 40/50\n",
      " - 226s - loss: 2.3012 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 41/50\n",
      " - 226s - loss: 2.3013 - acc: 0.1131 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 42/50\n",
      " - 226s - loss: 2.3012 - acc: 0.1114 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 43/50\n",
      " - 227s - loss: 2.3012 - acc: 0.1129 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 44/50\n",
      " - 225s - loss: 2.3005 - acc: 0.1151 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 45/50\n",
      " - 226s - loss: 2.3018 - acc: 0.1094 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 46/50\n",
      " - 226s - loss: 2.3013 - acc: 0.1128 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 47/50\n",
      " - 226s - loss: 2.3008 - acc: 0.1144 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 48/50\n",
      " - 226s - loss: 2.3012 - acc: 0.1126 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 49/50\n",
      " - 226s - loss: 2.3014 - acc: 0.1110 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 50/50\n",
      " - 226s - loss: 2.3012 - acc: 0.1129 - val_loss: 2.3011 - val_acc: 0.1135\n"
     ]
    }
   ],
   "source": [
    "#  Fit CNN model on MNIST data with image augmentation\n",
    "# *** RUN MORE EPOCHS WHEN TESTING FOR REAL ***\n",
    "epochs = 50\n",
    "CNN_hist4 = CNNmodel2.fit_generator(dgen, steps_per_epoch=len(X_train_kaggle)//128,\n",
    "                              validation_data=(X_test, Y_test), epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the CNN network on the test data\n",
      "[2.3010681228637697, 0.1135]\n"
     ]
    }
   ],
   "source": [
    "print (\"Evaluate the CNN network on the test data\")\n",
    "CNN2_Score = CNNmodel2.evaluate(X_test, Y_test, verbose=2)\n",
    "print (CNN2_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating test predictions...\")\n",
    "preds = CNNmodel2.predict_classes(X_test_kaggle, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_preds(preds, fname):\n",
    "    pd.DataFrame({\"ImageId\": list(range(1,len(preds)+1)), \"Label\": preds}).to_csv(fname, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_preds(preds, \"minst_kaggle_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
